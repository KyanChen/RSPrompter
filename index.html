<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <!--
    <script src="./resources/jsapi" type="text/javascript"></script>
    <script type="text/javascript" async>google.load("jquery", "1.3.2");</script>
   -->
    <style type="text/css">
        @font-face {
            font-family: 'Avenir Book';
            src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
        }
        body {
            font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
            font-weight:300;
            font-size:14px;
            margin-left: auto;
            margin-right: auto;
            width: 1200px;
        }
        h1 {
            font-weight:300;
        }
        h2 {
            font-weight:300;
        }

        p {
            font-weight:300;
            line-height: 1.4;
        }

        code {
            font-size: 0.8rem;
            margin: 0 0.2rem;
            padding: 0.5rem 0.8rem;
            white-space: nowrap;
            background: #efefef;
            border: 1px solid #d3d3d3;
            color: #000000;
            border-radius: 3px;
        }

        pre > code {
            display: block;
            white-space: pre;
            line-height: 1.5;
            padding: 0;
            margin: 0;
        }

        pre.prettyprint > code {
            border: none;
        }


        .container {
            display: flex;
            align-items: center;
            justify-content: center
        }
        .image {
            flex-basis: 40%
        }
        .text {
            padding-left: 20px;
            padding-right: 20px;
        }

        .disclaimerbox {
            background-color: #eee;
            border: 1px solid #eeeeee;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
            padding: 20px;
        }

        video.header-vid {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }

        img.header-img {
            height: 140px;
            border: 1px solid black;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;
        }

        img.rounded {
            border: 0px solid #eeeeee;
            border-radius: 10px ;
            -moz-border-radius: 10px ;
            -webkit-border-radius: 10px ;

        }

        a:link,a:visited
        {
            color: #1367a7;
            text-decoration: none;
        }
        a:hover {
            color: #208799;
        }

        td.dl-link {
            height: 160px;
            text-align: center;
            font-size: 22px;
        }

        .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow:
                    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                    5px 5px 0 0px #fff, /* The second layer */
                    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                    10px 10px 0 0px #fff, /* The third layer */
                    10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                    15px 15px 0 0px #fff, /* The fourth layer */
                    15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                    20px 20px 0 0px #fff, /* The fifth layer */
                    20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                    25px 25px 0 0px #fff, /* The fifth layer */
                    25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
            margin-left: 10px;
            margin-right: 45px;
        }


        .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
            box-shadow:
                    0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                    5px 5px 0 0px #fff, /* The second layer */
                    5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                    10px 10px 0 0px #fff, /* The third layer */
                    10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
            margin-top: 5px;
            margin-left: 10px;
            margin-right: 30px;
            margin-bottom: 5px;
        }

        .vert-cent {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
        }

        hr
        {
            border: 0;
            height: 1px;
            background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
        }
    </style>
    <title>RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model</title>
</head>

<body>
<br>
<span style="font-size:36px">
    <div style="text-align: center;">
        RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model
    </div>
</span>
<br>
<br>
<br>
<table align="center" width="1200px">
    <tr>
        <td align="center" width="160px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://kyanchen.github.io/">Keyan Chen</a><sup>1,2,3,4</sup></span>
            </div>
        </td>
        <td align="center" width="160px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://github.com/Chen-Yang-Liu">Chenyang Liu</a><sup>1,2,3,4</sup></span>
            </div>
        </td>

        <td align="center" width="160px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://github.com/justchenhao">Hao Chen</a><sup>1,2,3,4</sup></span>
            </div>
        </td>

        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://github.com/WindVChen">Haotian Zhang</a><sup>1,2,3,4</sup></span>
            </div>
        </td>

        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://github.com/zmoka-zht">Wenyuan Li</a><sup>1,2,3,4</sup></span>
            </div>
        </td>

        <td align="center" width="140px">
            <div style="text-align: center;">
                <span style="font-size:16px"><a href="https://zhengxiazou.github.io/">Zhengxia Zou</a><sup>1,4</sup></span>
            </div>
        </td>

        <td align="center" width="180px">
            <div style="text-align: center;">
                <span style="font-size:16px">
                    <a href="https://levir.buaa.edu.cn/">Zhenwei Shi</a>
                    <!--                    <sup><img class="round" style="width:20px" src="./resources/corresponding_fig.png">3</sup>-->
                    <sup>&#9993 1,2,3,4</sup>
                </span>
            </div>
        </td>
    </tr>
</table>

<br>

<table align="center" width="1000px">
    <tbody>
    <tr>
        <td align="center" width="100px">
            <center>
                <span style="font-size:16px">Beihang University<sup>1</sup></span>
            </center>
        </td>

        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">Beijing Key Laboratory of Digital Media<sup>2</sup></span>
            </center>
        </td>

        <td align="center" width="400px">
            <center>
                <span style="font-size:16px">State Key Laboratory of Virtual Reality Technology and Systems<sup>3</sup></span>
            </center>
        </td>

        <td align="center" width="300px">
            <center>
                <span style="font-size:16px">Shanghai Artificial Intelligence Laboratory<sup>4</sup></span>
            </center>
        </td>

    </tr>
    </tbody>
</table>


<!--<table align="center" width="700px">-->
<!--    <tbody>-->
<!--    <tr>-->
<!--        <td align="center" width="300px">-->
<!--            <center>-->
<!--                <span style="font-size:16px"><sup>&star;</sup>equal contribution</span>-->
<!--            </center>-->
<!--        </td>-->

<!--        <td align="center" width="300px">-->
<!--            <center>-->
<!--                <span style="font-size:16px"><sup>&#9993</sup>corresponding author</span>-->
<!--            </center>-->
<!--        </td>-->
<!--    </tr>-->
<!--    </tbody>-->
<!--</table>-->

<br>
<table align="center" width="700px">
    <tbody>
    <tr>
        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Code
                    <a href="https://github.com/KyanChen/RSPrompter">[GitHub]</a>
                </span>
            </div>
        </td>

        <td align="center" width="240px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Demo
                    <a href="https://huggingface.co/spaces/KyanChen/RSPrompter">[HuggingFace]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <div style="text-align: center;">
                <span style="font-size:20px">
                    Paper
                    <a href="https://arxiv.org/abs/2306.16269">[arXiv]</a>
                </span>
            </div>
        </td>

        <td align="center" width="200px">
            <center>
                <span style="font-size:20px">
                    Cite <a href="resources/cite.txt"> [BibTeX]</a>
                </span>
            </center>
        </td>
    </tr>
    </tbody>
</table>
<br>
<hr>

<div style="text-align: center;">
    <h2>Teaser</h2>
</div>


<p style="text-align:justify; text-justify:inter-ideograph;">
<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/teaser.jpg" width="800px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                (a) Depicts the instance segmentation results from point-based prompt, box-based prompt, SAM's "everything" mode (which segments all objects in the image), and RSPrompter. SAM performs category-agnostic instance segmentation, relying on manually provided prior prompts.
                (b) Illustrates the segmentation results of point-based prompts from different locations, a two-point based prompt, and a box-based prompt. The type, location, and quantity of prompts heavily influence SAM's results.
            </p>
        </td>
    </tr>
</table>


<br>
<hr>
<div style="text-align: center;">
    <h2>Abstract</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                Leveraging vast training data (SA-1B), the foundation Segment Anything Model (SAM) proposed by Meta AI Research exhibits remarkable generalization and zero-shot capabilities.
                Nonetheless, as a category-agnostic instance segmentation method, SAM heavily depends on prior manual guidance involving points, boxes, and coarse-grained masks.
                Additionally, its performance on remote sensing image segmentation tasks has yet to be fully explored and demonstrated.
                In this paper, we consider designing an automated instance segmentation approach for remote sensing images based on the SAM foundation model, incorporating semantic category information.
                Inspired by prompt learning, we propose a method to learn the generation of appropriate prompts for SAM input. This enables SAM to produce semantically discernible segmentation results for remote sensing images, which we refer to as RSPrompter.
                We also suggest several ongoing derivatives for instance segmentation tasks, based on recent developments in the SAM community, and compare their performance with RSPrompter.
                Extensive experimental results on the WHU building, NWPU VHR-10, and SSDD datasets validate the efficacy of our proposed method.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Architecture</h2>
</div>

<table>
    <tr>
        <td>
            <div style="text-align: center;">
                <img src="resources/model_gallery.jpg" width="1000px">
            </div>
        </td>
    </tr>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <br>
                From left to right, the figure illustrates SAM-seg, SAM-cls, SAM-det, and RSPrompter as alternative solutions for applying SAM to remote sensing image instance segmentation tasks.
                (a) An instance segmentation head is added after SAM's image encoder.
                (b) SAM's "everything" mode generates masks for all objects in an image, which are subsequently classified into specific categories by a classifier.
                (c) Object bounding boxes are first produced by an object detector and then used as prior prompts input to SAM to obtain the corresponding masks.
                (d) The proposed RSPrompter in this paper creates category-relevant prompt embeddings for instant segmentation masks.
                The snowflake symbol in the figure signifies that the model parameters in this part are kept frozen.
            </p>
        </td>
    </tr>
</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Quantitative Results</h2>
</div>

<table>
    <tr>
        <td>
            <p>
                <b>
                    R1: Benchmark on WHU
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                The results of RSPrompter in comparison to other methods on the WHU dataset are presented in Tab., with the best performance highlighted in bold.
                The task involves performing single-class instance segmentation of buildings in optical RGB band remote sensing images.
                RSPrompter-query attains the best performance for both box and mask predictions,
                achieving APbox and APmask values of 70.36/69.21.
                Specifically, SAM-seg (Mask2Former) surpasses the original Mask2Former (60.40/62.77) with 67.84/66.66 on APbox and APmask, while SAM-seg (Mask R-CNN) exceeds the original Mask R-CNN (56.11/60.75) with 67.15/64.86.
                Furthermore, both RSPrompter-query and RSPrompter-anchor improve the performance to 70.36/69.21 and 68.06/66.89, respectively, outperforming SAM-det, which carries out detection before segmentation.

                These observations suggest that the learning-to-prompt approach effectively adapts SAM for instance segmentation tasks in optical remote sensing images. Moreover, they demonstrate that the SAM backbone, trained on an extensive dataset, can provide valuable instance segmentation guidance even when it is fully frozen (as seen in SAM-seg).
            </p>
            <div style="text-align: center;">
                <img src="resources/vis_whu_tab.png" width="900px">
            </div>
        </td>
    </tr>


    <tr>
        <td>
            <br>
            <p>
                <b>
                    R2: Benchmark on NWPU
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                We conduct comparison experiments on the NWPU dataset to further validate RSPrompter's effectiveness.
                Unlike the WHU dataset, this one is smaller in size but encompasses more instance categories,
                amounting to 10 classes of remote sensing objects.
                The experiment remains focused on optical RGB band remote sensing image instance segmentation.
                Tab. exhibits the overall results of various methods on this dataset.

                It can be observed that RSPrompter-anchor, when compared to other approaches, generates the best results on box and mask predictions (68.54/67.64).
                In comparison to Mask R-CNN-based methods, single-stage methods display a substantial decline in performance on this dataset,
                particularly the Transformer-based Mask2Former. This may be because the dataset is relatively small, making it challenging for single-stage methods to achieve adequate generalization across the full data domain,
                especially for Transformer-based methods that require a large amount of training data.
                Nonetheless, it is worth noting that the performance of SAM-based SAM-seg (Mask2Former) and RSPrompter-query remains impressive.
                The performance improves from 29.60/35.02 for Mask2Former to 40.56/45.11 for SAM-seg (Mask2Former) and further to 58.79/65.29 for RSPrompter-query.

                These findings imply that SAM, when trained on a large amount of data, can exhibit significant generalization ability on a small dataset.
                Even when there are differences in the image domain, SAM's performance can be enhanced through the learning-to-prompt approach.
            </p>
            <div style="text-align: center;">
                <img src="resources/vis_nwpu_tab.png" width="900px">
            </div>
        </td>
    </tr>

    <tr>
        <td>
            <br>
            <p>
                <b>
                    R2: Benchmark on SSDD
                </b>
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                In this study, we carried out an evaluation of the SSDD dataset to thoroughly assess the capability of SAMpromper in performing remote sensing image instance segmentation tasks.
                The SSDD dataset is a single-category SAR ship instance segmentation dataset, representing a distinctly different modality compared to the previously mentioned datasets and exhibiting significant variations in training data from SAM.
                Tab. displays the AP values obtained for different methods on the dataset.

                It can be observed that the SAM-seg (Mask2Former) (49.08/54.03) and SAM-seg (Mask R-CNN) (62.41/59.46) approaches,
                which are based on the SAM backbone, demonstrate lower performance compared to the original Mask2Former (53.40/56.52) and Mask R-CNN (63.40/60.92).
                This suggests a considerable disparity between the SAM training image domain and the SAR data domain.
                Nonetheless, SAM-det achieved a significant performance enhancement (69.42/64.09), indicating that employing a detection-first approach followed by SAM for segmentation can yield high performance in SAM's cross-domain generalization.
                This is because SAM can provide accurate, category-agnostic, and highly generalized segmentation results with fine-grained intervention prompts.
                By unlocking the constrained space, RSPrompter-anchor further enhanced the performance to 73.09/72.61, thereby confirming the effectiveness of RSPrompter as well.
            </p>
            <div style="text-align: center;">
                <img src="resources/vis_ssdd_tab.png" width="900px">
            </div>
        </td>
    </tr>

</table>

<br>
<hr>
<div style="text-align: center;">
    <h2>Visualizations</h2>
</div>

<table>
    <tr>
        <td>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                To facilitate a more effective visual comparison with other methods,
                we present a qualitative analysis of the segmentation results obtained from SAM-based techniques and other state-of-the-art instance segmentation approaches.
                The following figures depict sample segmentation instances from the WHU dataset, NWPU dataset, and SSDD dataset, respectively.

                It can be observed that the proposed RSPrompter yields notable visual improvements in instance segmentation.
                Compared to alternative methods, the RSPrompter generates superior results, exhibiting sharper edges, more distinct contours, enhanced completeness, and a closer resemblance to the ground-truth references.
            </p>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <strong>Visualizations of the segmentation results obtained from the WHU dataset</strong>
            </p>
            <div style="text-align: center;">
                <img src="resources/vis_whu.jpg" width="1200px">
            </div>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <strong>Visualizations of the segmentation results obtained from the NWPU dataset</strong>
            </p>
            <div style="text-align: center;">
                <img src="resources/vis_nwpu.jpg" width="1200px">
            </div>
            <p style="text-align:justify; text-justify:inter-ideograph;">
                <strong>Visualizations of the segmentation results obtained from the SSDD dataset</strong>
            </p>
            <div style="text-align: center;">
                <img src="resources/vis_ssdd.jpg" width="1200px">
            </div>
        </td>
    </tr>

</table>

<br>
<hr>

<div style="text-align: center;">
    <h2>Acknowledgements</h2>
</div>
<p>
    Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
</p>

<br>
<br>
<br>

</body>
</html>
